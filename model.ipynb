{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain transformers\n",
    "!pip3 install langchain_community\n",
    "!pip3 install PyPDF2\n",
    "!pip3 install pypdf\n",
    "!pip3 install sentence-transformers\n",
    "!pip3 install chromadb\n",
    "!pip3 install langchain_together\n",
    "!pip3 install streamlit\n",
    "!pip3 install einops\n",
    "!pip3 install faiss-gpu\n",
    "!pip3 install faiss-cpu\n",
    "!pip3 install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_together import Together\n",
    "import os\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import streamlit as st\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ THE PDF FROM THE FOLDER\n",
    "# loader = DirectoryLoader('sample_data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)   #for multiple  pdfs\n",
    "# MEDICAL.PDF IS OF >5000 PAGES IT WILL TAKE TIME TOO MUCH TIME FOR EMBEDDING SO USE CMDT-2023.PDF\n",
    "loader = DirectoryLoader('sample_data', glob=\"CMDT-2023.pdf\", loader_cls=PyMuPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE ONLY WHEN UR DATA CONTAINS LOTS OF TABLES , COLORED BOXES , IMAGES ETC\n",
    "import pdfplumber\n",
    "\n",
    "# Function to extract text and tables from a PDF using pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    all_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from the page\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                all_text.append(text)\n",
    "\n",
    "            # Extract tables from the page and convert to text\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                # Handle potential None values within table rows\n",
    "                table_text = '\\n'.join(['\\t'.join([str(cell) if cell is not None else '' for cell in row]) for row in table])\n",
    "                all_text.append(table_text)\n",
    "\n",
    "    return '\\n'.join(all_text)\n",
    "\n",
    "# Load the PDF file and extract text\n",
    "pdf_path = \"sample_data/CMDT-2023.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Convert the extracted text into documents for further processing\n",
    "documents = [{\"page_content\": pdf_text}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE YOUR LOADED DATA\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING CHUNKS For unstructured text documents,USE “Recursive Character Splitting” strategy.\n",
    "#This strategy excels at preserving semantic coherence in the resulting fragments, effectively adapting to various types of\n",
    "#documents while avoiding the loss of relevant information.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0] # to check the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDINGS TO MAKE VECTOR DATABASE USING HUUGING FACE AS OPEN SOURCE FREE EMBEDDINGS\n",
    "embedings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "                                  # BAAI/bge-small-en-v1.5  or sentence-transformers/all-MiniLM-16-v2\n",
    "                                  model_kwargs={\"trust_remote_code\":True,\"revision\":\"289f532e14dbbbd5a04753fa58739e9ba766f3c7\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see Embeddings\n",
    "import numpy as np\n",
    "np.array(embedings.embed_query(texts[0].page_content)) # to check the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStore Creation USE ONLY WHEN U HAVE TO CREATE NEW VECTOR DATABASE\n",
    "faiss_db = FAISS.from_documents(texts, embedings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves and export the vector embeddings databse\n",
    "faiss_db.save_local(\"ipc_vector_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same here as model.py\n",
    "def reset_conversation():\n",
    "  st.session_state.messages = []\n",
    "  st.session_state.memory.clear()\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "    \n",
    "if \"memory\" not in st.session_state:\n",
    "    st.session_state.memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "                                   model_kwargs={\"trust_remote_code\":True,\"revision\":\"289f532e14dbbbd5a04753fa58739e9ba766f3c7\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = FAISS.load_local(\"ipc_vector_db_cmdt\", embeddings, allow_dangerous_deserialization=True)\n",
    "db = FAISS.load_local(\"ipc_vector_db_med\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_retriever = db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ACTUALLY TELLING CHATBOT WHAT U ARE AND CHAIN WHAT U HAVE TO DO\n",
    "# FOR CMDT-2023 DATA\n",
    "# prompt_template = \"\"\"<s>[INST]You are a medical chatbot trained on the latest data in diagnosis and treatment, designed to provide accurate and concise information in response to users' medical queries. Your primary focus is to offer evidence-based answers related to symptoms, infections, disorders, diseases, and their respective treatments. Refrain from generating hypothetical diagnoses or questions, and stick strictly to the context provided. Ensure your responses are professional, concise, and relevant. If the question falls outside the given context, do not rely on chat history; instead, generate an appropriate response based on your medical knowledge. Prioritize the user's query, avoid unnecessary details, and ensure compliance with medical standards and guidelines.\n",
    "# CONTEXT: {context}\n",
    "# CHAT HISTORY: {chat_history}\n",
    "# QUESTION: {question}\n",
    "# ANSWER:\n",
    "# </s>[INST]\n",
    "# \"\"\"\n",
    "\n",
    "## FOR MEDICAL DATA\n",
    "prompt_template = \"\"\"<s>[INST]You are a medical chatbot trained on the latest data in diagnosis and treatment from HARRISON'S PRINCIPLES OF INTERNAL MEDICINE. Your primary focus is to provide accurate, evidence-based answers related to symptoms, infections, disorders, diseases, and their respective treatments, including medications, cautionary advice, and necessary evaluations. Refrain from generating hypothetical diagnoses or questions, and strictly adhere to the context provided by the user’s query. Ensure your responses are professional, concise, and aligned with established medical standards and guidelines. If the question falls outside the provided context, do not rely on chat history; instead, generate an appropriate response based on your medical knowledge. Always prioritize the user's query, avoid unnecessary details, and maintain clarity in your explanations.\n",
    "CONTEXT: {context}\n",
    "CHAT HISTORY: {chat_history}\n",
    "QUESTION: {question}\n",
    "ANSWER:\n",
    "</s>[INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=['context', 'question', 'chat_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use other LLMs options from https://python.langchain.com/docs/integrations/llms. Here I have used TogetherAI API\n",
    "llm = Together(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    together_api_key=\"63796cfbe489810e7341f4622447cf023df92e6c6f9d665777f374032ba50474\" # mine \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st.session_state.memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\",return_messages=True),\n",
    "    retriever=db_retriever,\n",
    "    combine_docs_chain_kwargs={'prompt': prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user for a query\n",
    "user_query = input(\"Ask your question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the user enters a query, process it\n",
    "if user_query:\n",
    "    # Get the response from the Conversational Retrieval Chain\n",
    "    response = qa({\"question\": user_query})\n",
    "\n",
    "    # Display the chatbot's response\n",
    "    print(\"Chatbot:\", response['answer'])\n",
    "\n",
    "    # Optionally store the conversation for reference\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response['answer']})\n",
    "\n",
    "# If you want to reset the conversation, you can call reset_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO RUN THE STREAMLIT APP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_together import Together\n",
    "import os\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import streamlit as st\n",
    "import time\n",
    "from dotenv import load_dotenv # load specific environment that been created\n",
    " \n",
    "load_dotenv()\n",
    "## Langsmith project tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Set up the Streamlit page configuration\n",
    "st.set_page_config(page_title=\"MedGPT\", layout=\"wide\")\n",
    "\n",
    "# Custom CSS for styling the app\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "    /* Main container for flexbox layout */\n",
    "    .main {\n",
    "        display: flex;\n",
    "    }\n",
    "    \n",
    "    /* Sidebar styling */\n",
    "    .sidebar {\n",
    "        width: 300px;\n",
    "        padding: 20px;\n",
    "        height: 100vh;\n",
    "        position: fixed;\n",
    "        background-color: #000000;\n",
    "        left: 0;\n",
    "        top: 0;\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        align-items: center;\n",
    "    }\n",
    "    \n",
    "    /* Main chat container styling */\n",
    "    .chat-container {\n",
    "        flex: 1;\n",
    "        padding: 20px;\n",
    "        margin-left: 300px;\n",
    "    }\n",
    "    \n",
    "    .stApp, .ea3mdgi6 {\n",
    "        background-color: #000000; /* right side bg color */\n",
    "    }\n",
    "    \n",
    "    div.stButton > button:first-child {\n",
    "        background-color: #ffd0d0;\n",
    "    }\n",
    "    div.stButton > button:active {\n",
    "        background-color: #ff6262;\n",
    "    }\n",
    "    \n",
    "    div[data-testid=\"stStatusWidget\"] div button {\n",
    "        display: none;\n",
    "    }\n",
    "    \n",
    "    /* Adjust top margin of the report view container */\n",
    "    .reportview-container {\n",
    "        margin-top: -2em;\n",
    "    }\n",
    "    \n",
    "    /* Hide various Streamlit elements */\n",
    "    #MainMenu {visibility: hidden;}\n",
    "    .stDeployButton {display:none;}\n",
    "    footer {visibility: hidden;}\n",
    "    #stDecoration {display:none;}\n",
    "    button[title=\"View fullscreen\"]{\n",
    "        visibility: hidden;\n",
    "    }\n",
    "    \n",
    "    /* Ensure the placeholder text is also visible */\n",
    "    .stTextInput > div > div > input::placeholder {\n",
    "        color: #666666 !important;\n",
    "    }\n",
    "    \n",
    "    .stChatMessage {\n",
    "        background-color: #28282B; /* chat message background color set to black */\n",
    "        color : #000000 !important;\n",
    "    }\n",
    "\n",
    "\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True,\n",
    ")\n",
    "\n",
    "# Create the sidebar\n",
    "with st.sidebar:\n",
    "    # Add logo to the sidebar\n",
    "    st.image(\"Black Bold Initial AI Business Logo.jpg\", width=200)\n",
    "    # Add title to the sidebar\n",
    "    st.title(\"MedGPT\")\n",
    "    # Add description to the sidebar\n",
    "    st.markdown(\"Your AI MEDICAL ASSISTANT\")\n",
    "\n",
    "# Main chat interface container\n",
    "st.markdown('<div class=\"chat-container\">', unsafe_allow_html=True)\n",
    "\n",
    "# Function to reset the conversation\n",
    "def reset_conversation():\n",
    "    st.session_state.messages = []\n",
    "    st.session_state.memory.clear()\n",
    "\n",
    "# Initialize session state for messages if not already present\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "\n",
    "# Initialize conversation memory\n",
    "if \"memory\" not in st.session_state:\n",
    "    st.session_state[\"memory\"] = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\",return_messages=True) \n",
    "\n",
    "# Set up embeddings for vector search\n",
    "embedings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1\",model_kwargs={\"trust_remote_code\":True,\"revision\":\"289f532e14dbbbd5a04753fa58739e9ba766f3c7\"})\n",
    "# Load the FAISS vector database\n",
    "#db = FAISS.load_local(\"./ipc_vector_db_cmdt\", embedings, allow_dangerous_deserialization=True)\n",
    "db = FAISS.load_local(\"./ipc_vector_db_med\", embedings, allow_dangerous_deserialization=True)\n",
    "\n",
    "db_retriever = db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 4})\n",
    "\n",
    "# Define the prompt template for the AI\n",
    "# THIS IS ACTUALLY TELLING CHATBOT WHAT U ARE AND CHAIN WHAT U HAVE TO DO\n",
    "# FOR CMDT-2023 DATA\n",
    "# prompt_template = \"\"\"<s>[INST]You are a medical chatbot trained on the latest data in diagnosis and treatment, designed to provide accurate and concise information in response to users' medical queries. Your primary focus is to offer evidence-based answers related to symptoms, infections, disorders, diseases, and their respective treatments. Refrain from generating hypothetical diagnoses or questions, and stick strictly to the context provided. Ensure your responses are professional, concise, and relevant. If the question falls outside the given context, do not rely on chat history; instead, generate an appropriate response based on your medical knowledge. Prioritize the user's query, avoid unnecessary details, and ensure compliance with medical standards and guidelines.\n",
    "# CONTEXT: {context}\n",
    "# CHAT HISTORY: {chat_history}\n",
    "# QUESTION: {question}\n",
    "# ANSWER:\n",
    "# </s>[INST]\n",
    "# \"\"\"\n",
    "\n",
    "## FOR MEDICAL DATA\n",
    "prompt_template = \"\"\"<s>[INST]You are a medical chatbot trained on the latest data in diagnosis and treatment from HARRISON'S PRINCIPLES OF INTERNAL MEDICINE. Your primary focus is to provide accurate, evidence-based answers related to symptoms, infections, disorders, diseases, and their respective treatments, including medications, cautionary advice, and necessary evaluations. Refrain from generating hypothetical diagnoses or questions, and strictly adhere to the context provided by the user’s query. Ensure your responses are professional, concise, and aligned with established medical standards and guidelines. If the question falls outside the provided context, do not rely on chat history; instead, generate an appropriate response based on your medical knowledge. Always prioritize the user's query, avoid unnecessary details, and maintain clarity in your explanations.\n",
    "CONTEXT: {context}\n",
    "CHAT HISTORY: {chat_history}\n",
    "QUESTION: {question}\n",
    "ANSWER:\n",
    "</s>[INST]\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "prompt = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=['context', 'question', 'chat_history'])\n",
    "\n",
    "# Set up the language model (LLM)\n",
    "llm = Together(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    together_api_key=\"63796cfbe489810e7341f4622447cf023df92e6c6f9d665777f374032ba50474\"\n",
    ")\n",
    "\n",
    "# Create the conversational retrieval chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\",return_messages=True),\n",
    "    retriever=db_retriever,\n",
    "    combine_docs_chain_kwargs={'prompt': prompt}\n",
    ")\n",
    "\n",
    "# Display previous messages\n",
    "for message in st.session_state.get(\"messages\", []):\n",
    "    with st.chat_message(message.get(\"role\")):\n",
    "        st.write(message.get(\"content\"))\n",
    "\n",
    "# Create the chat input\n",
    "input_prompt = st.chat_input(\"Write your Queries here.....\")#input text box for user to ask question\n",
    "\n",
    "# Handle user input\n",
    "if input_prompt:\n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(input_prompt)\n",
    "\n",
    "    # Add user message to session state\n",
    "    st.session_state.messages.append({\"role\":\"user\",\"content\":input_prompt})\n",
    "\n",
    "    # Generate and display AI response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.status(\"Introspecting 💡...\",expanded=True):\n",
    "            # Invoke the QA chain to get the response\n",
    "            result = qa.invoke(input=input_prompt)\n",
    "\n",
    "            message_placeholder = st.empty()\n",
    "\n",
    "            full_response = \"⚠️ **_Note: Information provided is accordance to current medical diagnosis & treatment 2023._** \\n\\n\\n\"\n",
    "        # Stream the response\n",
    "        for chunk in result[\"answer\"]:\n",
    "            full_response+=chunk\n",
    "            time.sleep(0.02)\n",
    "            \n",
    "            message_placeholder.markdown(full_response+\" ▌\")\n",
    "        # Add a button to reset the conversation\n",
    "        st.button('Reset All Chat 🗑️', on_click=reset_conversation)\n",
    "\n",
    "    # Add AI response to session state\n",
    "    st.session_state.messages.append({\"role\":\"assistant\",\"content\":result[\"answer\"]})\n",
    "\n",
    "# Close the chat container div\n",
    "st.markdown('</div>', unsafe_allow_html=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
